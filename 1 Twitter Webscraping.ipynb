{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project scrapped data using twitter API and analyzed data using sentiment analysis and machine learning.\n",
    "First of all, let's look at the web-scrapping part:\n",
    "import necessary packages critical to web-scrapping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv \n",
    "from twython import Twython \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Twitter API requires consumer key and token to legally get the data, however, it only allows individual account to scrap at most 200 tweets each day and it only gives you the tweets during the last 7 days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_key='' \n",
    "consumer_secret='' \n",
    "access_token_key='' \n",
    "access_token_secret='' \n",
    "twitter = Twython(consumer_key,consumer_secret, access_token_key, access_token_secret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main part of scrapping is made into a class with different defintions:\n",
    "1. Tell twitter that we want 200 tweets from these specific users for whatever timeline of these twitter users.\n",
    "2. Write files into csv.\n",
    "3. Filter tweets to the ones with keywords \"vaccine\" etc..., which are the ones we want from these accounts. In the meantime, get these filtered tweets' information such as favorite count, text of tweets, user_follower counts, etc..\n",
    "4. Get these senders' profile information.\n",
    "5. Call for full tweets from specified ids with these keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class web_scrapping twitter:\n",
    "    def get_user_timeline():\n",
    "        \"\"\"\n",
    "        Get all the tweets from the user list timeline\n",
    "        \"\"\"\n",
    "        users = [\"HelenBranswell\", \"CMSRIResearch\", \"EsotericExposal\", \"vaccinationmyth\", \"STHGibbs\"]\n",
    "        results = []\n",
    "        for user in users:\n",
    "            try:\n",
    "                this_timeline = twitter.get_user_timeline(screen_name=user, count=200)\n",
    "                results.extend(this_timeline)\n",
    "            except Exception:\n",
    "                print('There was an exception on: '+user)\n",
    "                continue\n",
    "        return results\n",
    "    def to_csv(df, file_name):\n",
    "        \"\"\"\n",
    "        Write file to csv \n",
    "        \"\"\"\n",
    "        writer = pd.CsvWriter(file_name)\n",
    "        df.to_csv(writer,'Sheet1', index=False)\n",
    "        writer.save()\n",
    "    def filter_tweets(results):\n",
    "        \"\"\"\n",
    "        Filter out tweets to only contain the following key words, no RT, no duplicate tweets, \n",
    "        and only contains the following fields. \n",
    "        \"\"\"\n",
    "        data = []\n",
    "        tweet_set = set()\n",
    "        keywords = ['vaccine', 'vaccines', 'vaccination','vaccinate', 'vaccinations']\n",
    "        for tweet in results:\n",
    "            try:\n",
    "                if is_valid(tweet, keywords):\n",
    "                    row  = {\n",
    "                    'id': tweet['id'],\n",
    "                    'text': str(tweet['text']).encode('utf-8'),\n",
    "                    'retweet_count': tweet['retweet_count'], \n",
    "                    'timestamp' :tweet['created_at'], \n",
    "                    'media': [], \n",
    "                    'favorite_count': tweet['favorite_count'], \n",
    "                    'user_follower_count': tweet['user']['followers_count'],\n",
    "                    'user_id': tweet['user']['id'], \n",
    "                    'user_screen_name' : str(tweet['user']['screen_name']).encode('utf-8'),\n",
    "                    'url' : [],\n",
    "                    'hash_tags': []\n",
    "                    }\n",
    "                    data.append(row)\n",
    "                tweet_set.add(tweet['id'])\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "        return data\n",
    "    def is_valid(tweet, keywords):\n",
    "        text = str(tweet['text'])\n",
    "        return 'RT' not in text and any([k in text for k in keywords])\n",
    "    def get_user_information():\n",
    "        \"\"\"\n",
    "        Get the profile of the user \n",
    "        \"\"\"\n",
    "        results = {}\n",
    "        users = [\"HelenBranswell\", \"CMSRIResearch\", \"EsotericExposal\", \"vaccinationmyth\", \"STHGibbs\"]\n",
    "        for user in users:\n",
    "            results['user'] = twitter.show_user(screen_name=user)\n",
    "        return results\n",
    "    def get_full_tweet(df):\n",
    "        \"\"\"\n",
    "        Get information of a list of tweet ids\n",
    "        \"\"\"\n",
    "        r = []\n",
    "        for i, tweet_id in enumerate(df['id']):\n",
    "            try:\n",
    "                tweet = twitter.show_status(id=str(tweet_id), tweet_mode='extended')\n",
    "                df.loc[i,'text'] = tweet['full_text']\n",
    "                r.append(tweet)\n",
    "            except Exception as e:\n",
    "                print(tweet_id, e)\n",
    "        return r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the results according to class functions.\n",
    "Get user first and scrap tweets with keywords from these users and get all these filtered tweets and define it as a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = get_user_timeline()\n",
    "results2 = filter_tweets(results)\n",
    "df = pd.DataFrame(results2)\n",
    "tweets = get_full_tweet(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print the scrapped data to see what does it look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     favorite_count hash_tags                   id media  retweet_count  \\\n",
      "0                 2        []  1005899221852147712    []              0   \n",
      "1                 9        []  1005853124861616128    []             12   \n",
      "2                15        []  1005493698489208833    []             51   \n",
      "3                51        []  1005481133507792896    []             36   \n",
      "4                 0        []  1005209205337415681    []              0   \n",
      "5                14        []  1005172926713384961    []             14   \n",
      "6                 0        []  1005170203909554180    []              0   \n",
      "7                 5        []  1005113336130830336    []              9   \n",
      "8                 3        []  1005051983634550786    []             14   \n",
      "9                 2        []  1004870459782025217    []              0   \n",
      "10               11        []  1004484833370877952    []              4   \n",
      "11                3        []  1004113140613832704    []              0   \n",
      "12                1        []  1003803246157287424    []              0   \n",
      "13                3        []  1003663288943742976    []              1   \n",
      "14               18        []  1003375826052710401    []             14   \n",
      "15                0        []  1002585949522546688    []              0   \n",
      "16                0        []  1002585625248305152    []              0   \n",
      "17               21        []  1001920269232082944    []              9   \n",
      "18               45        []  1001823726940360704    []             37   \n",
      "19               56        []  1004018287754272768    []             44   \n",
      "20                0        []  1003687091220381697    []              0   \n",
      "21               62        []  1000484742742003712    []             65   \n",
      "22                3        []  1000421021940404225    []              3   \n",
      "23                4        []  1000417423311474688    []              3   \n",
      "24                7        []  1000116051240157184    []             10   \n",
      "25               13        []   999996942900977664    []             13   \n",
      "26                4        []   999381992960659456    []              5   \n",
      "27               57        []   998670432436998146    []             46   \n",
      "28               22        []   998580968805027840    []             24   \n",
      "29               29        []   998260053365829632    []             33   \n",
      "..              ...       ...                  ...   ...            ...   \n",
      "85                1        []   886744822970810368    []              0   \n",
      "86                1        []   884896378434060288    []              1   \n",
      "87               11        []   884382998128451584    []              7   \n",
      "88                9        []   879674461510881280    []              4   \n",
      "89                0        []   879320911517802497    []              0   \n",
      "90                0        []   877651110726651904    []              2   \n",
      "91                1        []   876180171925180417    []              4   \n",
      "92                3        []   874389626823340032    []              2   \n",
      "93                7        []   872959034046939138    []              9   \n",
      "94                7        []   867528194097909762    []              9   \n",
      "95                1        []   867153238767931392    []              1   \n",
      "96                1        []   867036233331073024    []              1   \n",
      "97               14        []   865701132584312832    []             18   \n",
      "98                1        []   865684768331116545    []              0   \n",
      "99                1        []   865239348022792194    []              0   \n",
      "100               0        []   865207928281849858    []              1   \n",
      "101               0        []   864539723271987202    []              0   \n",
      "102              22        []   864466777694142465    []             20   \n",
      "103               2        []   863466415927545857    []              2   \n",
      "104               1        []   863421095021617152    []              1   \n",
      "105               1        []   862790695769300992    []              0   \n",
      "106               8        []   859915527346683911    []              6   \n",
      "107               3        []   859192000091967488    []              2   \n",
      "108              10        []   859166828668620800    []             10   \n",
      "109               2        []   857248380048822272    []              3   \n",
      "110               2        []   856495509057540098    []              1   \n",
      "111               2        []   854321187589107713    []              0   \n",
      "112               2        []   853954998203535361    []              1   \n",
      "113               4        []  1005592909259984898    []             16   \n",
      "114              18        []  1005589860709695488    []             44   \n",
      "\n",
      "                                                  text  \\\n",
      "0    3. #EbolaDRC: Another 76 people have been vacc...   \n",
      "1    Clarifying: A @WHO contact reached out to say ...   \n",
      "2    There is confirmation of a case of paralysis c...   \n",
      "3    1. Interesting #EbolaDRC epi curve in @WHO's l...   \n",
      "4    @Ourotheboros All the vaccines are made differ...   \n",
      "5    A @WHO expert group has been reviewing data th...   \n",
      "6    2. #EbolaDRC: another 238 people have received...   \n",
      "7    This move by China to bring their #Ebola vacci...   \n",
      "8    Chinese scientists are heading to #DRC with #E...   \n",
      "9    3. #EbolaDRC: Another 247 people have been vac...   \n",
      "10   3. #EbolaDRC: Another 210 people have been vac...   \n",
      "11   2. #EbolaDRC: Better news on the vaccine front...   \n",
      "12   3. Another 87 people in #DRC have been vaccina...   \n",
      "13   2. But #Ebola vaccination began at Iboko on Ma...   \n",
      "14   3. To date 1,112 people have been vaccinated w...   \n",
      "15   @ALichtenberg1 The #Merck VSV #Ebola vaccine i...   \n",
      "16   @ALichtenberg1 I. Companies are not currently ...   \n",
      "17   Update to my update: #DRC's daily #Ebola repor...   \n",
      "18   The reality is, the only time you can test if ...   \n",
      "19   It’s no wonder another study showed confidence...   \n",
      "20   @PhRMA Northwest Bio webcast of dendritic cell...   \n",
      "21   Ethical questions surround vaccine to reduce f...   \n",
      "22   Singapore, a country recently noted for its go...   \n",
      "23   Del Bigtree asks, How can the science be settl...   \n",
      "24   An ongoing series of admirably methodical stud...   \n",
      "25   #Ebola hype is a prelude to a new mandated #va...   \n",
      "26   Did you know #vaccine safety was a topic of an...   \n",
      "27   The first-of-its-kind study of #vaccinated vs....   \n",
      "28   We now have the putative link between vaccinat...   \n",
      "29   If you are concerned about the safety of too m...   \n",
      "..                                                 ...   \n",
      "85   It's the most rewarding work imaginable to hel...   \n",
      "86   \"When somebody close to you shares their story...   \n",
      "87   Government study : vaccinated babies 10x more ...   \n",
      "88   Breastmilk does what a vaccine can never do.  ...   \n",
      "89   Moving past the #vaccination #myth through the...   \n",
      "90   HPV vaccine Gardasil has a dark side, Star inv...   \n",
      "91   Vitamin C outperforms vaccines - https://t.co/...   \n",
      "92   If only half of America is properly vaccinated...   \n",
      "93   The measles vaccine is associated with ... #en...   \n",
      "94   Coincidence?  I think not.  My son was vaccine...   \n",
      "95   Do I need a tetanus shot? - Holistic Squid #te...   \n",
      "96   How Aluminum from vaccines gets transported in...   \n",
      "97   \"Until recently I had never considered vaccine...   \n",
      "98   Do you wonder whether you or someone you love ...   \n",
      "99   \"DPT: A Shot in the Dark\" and a brave visionar...   \n",
      "100  Do you suspect having a vaccine damaged child?...   \n",
      "101    Saying no to vaccines.  https://t.co/cJRpraVxKn   \n",
      "102  Harvard study proves unvaccinated children pos...   \n",
      "103  The Problem With the Titre-Count : Another Vac...   \n",
      "104  Excellent Resources for Individuals Researchin...   \n",
      "105  The deeper meaning behind the vaccine additive...   \n",
      "106  The statistics don't lie.  I used to be pro-va...   \n",
      "107  Aborted fetuses are in vaccines (https://t.co/...   \n",
      "108  The CDC claims the shingles vaccine is safe, b...   \n",
      "109  To listen to how we rescued our own son from v...   \n",
      "110  Dorit Rubinstein Reiss apologizes for her posi...   \n",
      "111  HP (Homoprophylaxis) and How to Get Your “Gree...   \n",
      "112  The contaminated #Salk polio vaccines given to...   \n",
      "113  The child affected had not been vaccinated. Th...   \n",
      "114  The Pan American Health Organisation confirms ...   \n",
      "\n",
      "                          timestamp url  user_follower_count     user_id  \\\n",
      "0    Sun Jun 10 19:47:17 +0000 2018  []                23038    28838219   \n",
      "1    Sun Jun 10 16:44:07 +0000 2018  []                23038    28838219   \n",
      "2    Sat Jun 09 16:55:53 +0000 2018  []                23038    28838219   \n",
      "3    Sat Jun 09 16:05:57 +0000 2018  []                23038    28838219   \n",
      "4    Fri Jun 08 22:05:25 +0000 2018  []                23038    28838219   \n",
      "5    Fri Jun 08 19:41:15 +0000 2018  []                23038    28838219   \n",
      "6    Fri Jun 08 19:30:26 +0000 2018  []                23038    28838219   \n",
      "7    Fri Jun 08 15:44:28 +0000 2018  []                23038    28838219   \n",
      "8    Fri Jun 08 11:40:40 +0000 2018  []                23038    28838219   \n",
      "9    Thu Jun 07 23:39:21 +0000 2018  []                23038    28838219   \n",
      "10   Wed Jun 06 22:07:01 +0000 2018  []                23038    28838219   \n",
      "11   Tue Jun 05 21:30:02 +0000 2018  []                23038    28838219   \n",
      "12   Tue Jun 05 00:58:38 +0000 2018  []                23038    28838219   \n",
      "13   Mon Jun 04 15:42:29 +0000 2018  []                23038    28838219   \n",
      "14   Sun Jun 03 20:40:13 +0000 2018  []                23038    28838219   \n",
      "15   Fri Jun 01 16:21:32 +0000 2018  []                23038    28838219   \n",
      "16   Fri Jun 01 16:20:14 +0000 2018  []                23038    28838219   \n",
      "17   Wed May 30 20:16:21 +0000 2018  []                23038    28838219   \n",
      "18   Wed May 30 13:52:44 +0000 2018  []                23038    28838219   \n",
      "19   Tue Jun 05 15:13:08 +0000 2018  []                 3594  2397798204   \n",
      "20   Mon Jun 04 17:17:04 +0000 2018  []                 3594  2397798204   \n",
      "21   Sat May 26 21:12:05 +0000 2018  []                 3594  2397798204   \n",
      "22   Sat May 26 16:58:53 +0000 2018  []                 3594  2397798204   \n",
      "23   Sat May 26 16:44:35 +0000 2018  []                 3594  2397798204   \n",
      "24   Fri May 25 20:47:02 +0000 2018  []                 3594  2397798204   \n",
      "25   Fri May 25 12:53:44 +0000 2018  []                 3594  2397798204   \n",
      "26   Wed May 23 20:10:09 +0000 2018  []                 3594  2397798204   \n",
      "27   Mon May 21 21:02:40 +0000 2018  []                 3594  2397798204   \n",
      "28   Mon May 21 15:07:10 +0000 2018  []                 3594  2397798204   \n",
      "29   Sun May 20 17:51:58 +0000 2018  []                 3594  2397798204   \n",
      "..                              ...  ..                  ...         ...   \n",
      "85   Mon Jul 17 00:30:15 +0000 2017  []                 2567    59868227   \n",
      "86   Tue Jul 11 22:05:11 +0000 2017  []                 2567    59868227   \n",
      "87   Mon Jul 10 12:05:12 +0000 2017  []                 2567    59868227   \n",
      "88   Tue Jun 27 12:15:09 +0000 2017  []                 2567    59868227   \n",
      "89   Mon Jun 26 12:50:16 +0000 2017  []                 2567    59868227   \n",
      "90   Wed Jun 21 22:15:05 +0000 2017  []                 2567    59868227   \n",
      "91   Sat Jun 17 20:50:06 +0000 2017  []                 2567    59868227   \n",
      "92   Mon Jun 12 22:15:07 +0000 2017  []                 2567    59868227   \n",
      "93   Thu Jun 08 23:30:27 +0000 2017  []                 2567    59868227   \n",
      "94   Wed May 24 23:50:14 +0000 2017  []                 2567    59868227   \n",
      "95   Tue May 23 23:00:17 +0000 2017  []                 2567    59868227   \n",
      "96   Tue May 23 15:15:21 +0000 2017  []                 2567    59868227   \n",
      "97   Fri May 19 22:50:08 +0000 2017  []                 2567    59868227   \n",
      "98   Fri May 19 21:45:07 +0000 2017  []                 2567    59868227   \n",
      "99   Thu May 18 16:15:10 +0000 2017  []                 2567    59868227   \n",
      "100  Thu May 18 14:10:19 +0000 2017  []                 2567    59868227   \n",
      "101  Tue May 16 17:55:07 +0000 2017  []                 2567    59868227   \n",
      "102  Tue May 16 13:05:15 +0000 2017  []                 2567    59868227   \n",
      "103  Sat May 13 18:50:10 +0000 2017  []                 2567    59868227   \n",
      "104  Sat May 13 15:50:05 +0000 2017  []                 2567    59868227   \n",
      "105  Thu May 11 22:05:06 +0000 2017  []                 2567    59868227   \n",
      "106  Wed May 03 23:40:12 +0000 2017  []                 2567    59868227   \n",
      "107  Mon May 01 23:45:10 +0000 2017  []                 2567    59868227   \n",
      "108  Mon May 01 22:05:09 +0000 2017  []                 2567    59868227   \n",
      "109  Wed Apr 26 15:01:55 +0000 2017  []                 2567    59868227   \n",
      "110  Mon Apr 24 13:10:16 +0000 2017  []                 2567    59868227   \n",
      "111  Tue Apr 18 13:10:18 +0000 2017  []                 2567    59868227   \n",
      "112  Mon Apr 17 12:55:11 +0000 2017  []                 2567    59868227   \n",
      "113  Sat Jun 09 23:30:07 +0000 2018  []                 5951   114587401   \n",
      "114  Sat Jun 09 23:18:00 +0000 2018  []                 5951   114587401   \n",
      "\n",
      "       user_screen_name  \n",
      "0     b'HelenBranswell'  \n",
      "1     b'HelenBranswell'  \n",
      "2     b'HelenBranswell'  \n",
      "3     b'HelenBranswell'  \n",
      "4     b'HelenBranswell'  \n",
      "5     b'HelenBranswell'  \n",
      "6     b'HelenBranswell'  \n",
      "7     b'HelenBranswell'  \n",
      "8     b'HelenBranswell'  \n",
      "9     b'HelenBranswell'  \n",
      "10    b'HelenBranswell'  \n",
      "11    b'HelenBranswell'  \n",
      "12    b'HelenBranswell'  \n",
      "13    b'HelenBranswell'  \n",
      "14    b'HelenBranswell'  \n",
      "15    b'HelenBranswell'  \n",
      "16    b'HelenBranswell'  \n",
      "17    b'HelenBranswell'  \n",
      "18    b'HelenBranswell'  \n",
      "19     b'CMSRIResearch'  \n",
      "20     b'CMSRIResearch'  \n",
      "21     b'CMSRIResearch'  \n",
      "22     b'CMSRIResearch'  \n",
      "23     b'CMSRIResearch'  \n",
      "24     b'CMSRIResearch'  \n",
      "25     b'CMSRIResearch'  \n",
      "26     b'CMSRIResearch'  \n",
      "27     b'CMSRIResearch'  \n",
      "28     b'CMSRIResearch'  \n",
      "29     b'CMSRIResearch'  \n",
      "..                  ...  \n",
      "85   b'vaccinationmyth'  \n",
      "86   b'vaccinationmyth'  \n",
      "87   b'vaccinationmyth'  \n",
      "88   b'vaccinationmyth'  \n",
      "89   b'vaccinationmyth'  \n",
      "90   b'vaccinationmyth'  \n",
      "91   b'vaccinationmyth'  \n",
      "92   b'vaccinationmyth'  \n",
      "93   b'vaccinationmyth'  \n",
      "94   b'vaccinationmyth'  \n",
      "95   b'vaccinationmyth'  \n",
      "96   b'vaccinationmyth'  \n",
      "97   b'vaccinationmyth'  \n",
      "98   b'vaccinationmyth'  \n",
      "99   b'vaccinationmyth'  \n",
      "100  b'vaccinationmyth'  \n",
      "101  b'vaccinationmyth'  \n",
      "102  b'vaccinationmyth'  \n",
      "103  b'vaccinationmyth'  \n",
      "104  b'vaccinationmyth'  \n",
      "105  b'vaccinationmyth'  \n",
      "106  b'vaccinationmyth'  \n",
      "107  b'vaccinationmyth'  \n",
      "108  b'vaccinationmyth'  \n",
      "109  b'vaccinationmyth'  \n",
      "110  b'vaccinationmyth'  \n",
      "111  b'vaccinationmyth'  \n",
      "112  b'vaccinationmyth'  \n",
      "113         b'STHGibbs'  \n",
      "114         b'STHGibbs'  \n",
      "\n",
      "[115 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "print ( df )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save it to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('result_1.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
